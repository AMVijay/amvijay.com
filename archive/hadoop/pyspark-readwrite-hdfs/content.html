<h1 id="apache-spark-data-processing-from-hdfs">Apache Spark Data Processing From HDFS</h1>
<p><strong>Use Case Requirement</strong> </p>
<ul>
<li>Load a file with large dataset into hdfs.</li>
<li>Use Spark to read the file from hdfs.</li>
<li>Process the data and create another dataset.</li>
<li>Write the processed new dataset into hdfs again.</li>
</ul>
<h2 id="load-a-large-data-set-into-hdfs">Load a Large Data Set into HDFS</h2>
<ul>
<li>Load the Flight Details data from <a href="https://www.kaggle.com/open-flights/flight-route-database">https://www.kaggle.com/open-flights/flight-route-database</a>. <ul>
<li>Required Kaggle Login Account to download this public data set.</li>
</ul>
</li>
<li>Make Sure HDFS is up and running. Otherwise refer <a href="https://amvijay.github.io/hadoop/hadoop-setup-steps/">https://amvijay.github.io/hadoop/hadoop-setup-steps/</a> to bring HDFS.</li>
<li>Load the dataset into HDFS, Command reference: <ul>
<li><code>hdfs dfs -ls /</code> - List the files inside root directory</li>
<li><code>hdfs dfs -mkdir /admin</code>- Create directory </li>
<li><code>hdfs dfs -mkdir /admin/input</code> - Create directory</li>
<li><code>hdfs dfs -put /home/admin/routes.csv /admin/input</code> - Copy the files from local to hdfs</li>
<li><code>hdfs dfs -mkdir /admin/output</code> - Create directory</li>
</ul>
</li>
</ul>
<h2 id="use-spark-to-read-the-large-dataset-file-from-hdfs">Use Spark to read the large dataset file from HDFS</h2>
<p>Below python code will read the data set from HDFS</p>
<pre><code>from pyspark import SparkConf, SparkContext

<span class="hljs-keyword">conf</span> = SparkConf().setAppName(<span class="hljs-string">"Test Application"</span>).setMaster(<span class="hljs-string">"spark://127.0.1.1:7077"</span>)
spark_context = SparkContext(<span class="hljs-keyword">conf</span>=<span class="hljs-keyword">conf</span>)

lines = spark_context.textFile(<span class="hljs-string">"hdfs://localhost:9000/admin/input/routes.csv"</span>)

<span class="hljs-keyword">for</span> <span class="hljs-built_in">line</span> in <span class="hljs-built_in">line</span><span class="hljs-variable">s:</span>
    <span class="hljs-keyword">print</span>(<span class="hljs-built_in">line</span>)
</code></pre><h2 id="process-the-dataset-and-create-another-dataset-using-spark">Process the dataset and create another dataset Using Spark</h2>
<p>Added map() and filter() method call with lambda implementation to read the CSV file and filter the data to create new RDD data set by applying filter to get flights details has destination as LAX.</p>
<pre><code>from pyspark import SparkConf, SparkContext

<span class="hljs-keyword">conf</span> = SparkConf().setAppName(<span class="hljs-string">"Test Application"</span>).setMaster(<span class="hljs-string">"spark://127.0.1.1:7077"</span>)
spark_context = SparkContext(<span class="hljs-keyword">conf</span>=<span class="hljs-keyword">conf</span>)

lines = spark_context.textFile(<span class="hljs-string">"hdfs://localhost:9000/admin/input/routes.csv"</span>).<span class="hljs-keyword">map</span>(lambda <span class="hljs-built_in">line</span>: <span class="hljs-built_in">line</span>.<span class="hljs-keyword">split</span>(<span class="hljs-string">","</span>)).<span class="hljs-built_in">filter</span>(lambda column: column[<span class="hljs-number">4</span>] == <span class="hljs-string">'LAX'</span>).collect()

<span class="hljs-keyword">for</span> <span class="hljs-built_in">line</span> in <span class="hljs-built_in">line</span><span class="hljs-variable">s:</span>
    <span class="hljs-keyword">print</span>(<span class="hljs-built_in">line</span>)

<span class="hljs-keyword">print</span>(<span class="hljs-string">"Ended the Execution"</span>)
</code></pre><h2 id="write-the-new-dataset-into-hdfs">Write the new dataset into HDFS</h2>
<p>Added saveAsTextFile() method to write the new RDD in hdfs.</p>
<pre><code>from pyspark import SparkConf, SparkContext

<span class="hljs-keyword">conf</span> = SparkConf().setAppName(<span class="hljs-string">"Test Application"</span>).setMaster(<span class="hljs-string">"spark://127.0.1.1:7077"</span>)
spark_context = SparkContext(<span class="hljs-keyword">conf</span>=<span class="hljs-keyword">conf</span>)

lines = spark_context.textFile(<span class="hljs-string">"hdfs://localhost:9000/admin/input/routes.csv"</span>).<span class="hljs-keyword">map</span>(lambda <span class="hljs-built_in">line</span>: <span class="hljs-built_in">line</span>.<span class="hljs-keyword">split</span>(<span class="hljs-string">","</span>)).<span class="hljs-built_in">filter</span>(lambda column: column[<span class="hljs-number">4</span>] == <span class="hljs-string">'LAX'</span>).saveAsTextFile(<span class="hljs-string">"hdfs://localhost:9000/admin/output/lax-routes"</span>)

<span class="hljs-keyword">for</span> <span class="hljs-built_in">line</span> in <span class="hljs-built_in">line</span><span class="hljs-variable">s:</span>
    <span class="hljs-keyword">print</span>(<span class="hljs-built_in">line</span>)

<span class="hljs-keyword">print</span>(<span class="hljs-string">"Ended the Execution"</span>)
</code></pre><h2 id="execute-this-python-script-file-using-spark-submit">Execute this python script file using spark-submit</h2>
<p>Execute the the python file using spark-submit
<code>sudo /usr/local/spark/bin/spark-submit --master spark://LP0039973:7077 /mnt/c/Users/amvij/Vijay/github/hadoop-learning/spark/spark-context.py</code></p>
