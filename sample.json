{
  "content": "<h1 id=\"apache-sqoop-setup-configuration-and-load-data-to-hadoop\">Apache Sqoop Setup, Configuration, and Load Data to HADOOP</h1>\n<ul>\n<li>Last Updated: 02/21/2020</li>\n</ul>\n<h2 id=\"pre-requisite\">Pre-requisite</h2>\n<ul>\n<li>Need to have Hadoop Installed already. Refer <a href=\"https://amvijay.github.io/hadoop/hadoop-setup-steps\">https://amvijay.github.io/hadoop/hadoop-setup-steps</a> for Hadoop Installation details.</li>\n</ul>\n<h2 id=\"sqoop-setup-steps\">Sqoop Setup Steps</h2>\n<ul>\n<li>Download the Sqoop using wget command <code>wget http://mirror.olnevhost.net/pub/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz --no-check-certificate</code></li>\n<li>Extract the sqoop bonaries using command <code>tar -xvzf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</code></li>\n<li>Move the sqoop binary folder to /usr/local directory using command <code>sudo mv sqoop-1.4.7.bin__hadoop-2.6.0 /usr/local/sqoop-1.4.7</code></li>\n<li>Create softlink /usr/local/sqoop using command <code>sudo ln -s /usr/local/sqoop-1.4.7 /usr/local/sqoop</code></li>\n</ul>\n<h2 id=\"sqoop-configuration-with-hadoop\">Sqoop Configuration with HADOOP</h2>\n<ul>\n<li>Create the sqoop-env.sh from template using command <code>sudo cp /usr/local/sqoop/conf/sqoop-env-template.sh /usr/local/sqoop/conf/sqoop-env.sh</code></li>\n<li>Update the sqoop-env.sh using command <code>sudo nano /usr/local/sqoop/conf/sqoop-env.sh</code> with environment variable path <pre><code class=\"lang-bash\"><span class=\"hljs-built_in\">export</span> HADOOP_COMMON_HOME=/usr/<span class=\"hljs-built_in\">local</span>/hadoop\n<span class=\"hljs-built_in\">export</span> HADOOP_MAPRED_HOME=/usr/<span class=\"hljs-built_in\">local</span>/hadoop\n</code></pre>\n</li>\n<li>Download the oracle jdbc jar files and copy into <code>/usr/local/sqoop/lib</code> directory.<ul>\n<li>oracle jdbc jars needed to be downloaded it from oracle <a href=\"https://www.oracle.com/database/technologies/appdev/jdbc-downloads.html\">https://www.oracle.com/database/technologies/appdev/jdbc-downloads.html</a></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"test-sqoop-connection-with-oracle-database\">Test Sqoop connection with Oracle Database</h2>\n<ul>\n<li>Use command <code>sudo /usr/local/sqoop/bin/sqoop list-tables --connect jdbc:oracle:thin:@AU4T.tmm.na.corp.toyota.com:1533:AU4T --username IPLUS_TEST_FR3_OLTP --password changeit</code> to test whether sqoop is able to connect to oracle database and list tables.</li>\n</ul>\n<h2 id=\"import-table-data-into-hadoop\">Import Table Data into HADOOP</h2>\n<ul>\n<li>Create directory in hdfs as below<ul>\n<li><code>/usr/local/hadoop/bin/hdfs dfs -mkdir /sqoop</code></li>\n<li><code>/usr/local/hadoop/bin/hdfs dfs -mkdir /sqoop/import</code></li>\n<li>Here, we need to run this create directory using same user id in which hdfs process running.</li>\n</ul>\n</li>\n<li>Make sure HDFS File System and YARN are running before sqoop import starts.</li>\n<li>Use command <code>/usr/local/sqoop/bin/sqoop import --connect jdbc:oracle:thin:@AU4T.tmm.na.corp.toyota.com:1533:AU4T --username IPLUS_TEST_FR3_OLTP --password changeit --table OPTION_TYPE --target-dir /sqoop/import/option_type</code></li>\n</ul>\n<h3 id=\"error-and-resolution-while-importing-\">Error and Resolution while importing:</h3>\n<ul>\n<li><p><strong>Error</strong> Error Trace: <code>java.lang.NoClassDefFoundError: org/apache/commons/lang/StringUtils</code></p>\n<ul>\n<li>Resolution: Added <code>commons-lang-2.6.jar</code> to sqoop installation library path <code>/usr/local/sqoop/lib</code>. This issue resolved.</li>\n</ul>\n</li>\n<li><p><strong>Error</strong> 2020-02-19 21:33:29,520 ERROR tool.ImportTool: Import failed: There is no column found in the target table option_type. Please ensure that your table name is correct</p>\n<ul>\n<li>Resolution: When we pass table in lower case, the above stated error comes. so corrected the import command from \n<code>/usr/local/sqoop/bin/sqoop import --connect jdbc:oracle:thin:@AU4T.tmm.na.corp.toyota.com:1533:AU4T --username IPLUS_TEST_FR3_OLTP --password changeit --table option_type --target-dir /sqoop/import</code>\nto\n<code>/usr/local/sqoop/bin/sqoop import --connect jdbc:oracle:thin:@AU4T.tmm.na.corp.toyota.com:1533:AU4T --username IPLUS_TEST_FR3_OLTP --password changeit --table OPTION_TYPE --target-dir /sqoop/import</code></li>\n</ul>\n</li>\n<li><p><strong>Error</strong> 2020-02-19 21:48:22,153 ERROR tool.ImportTool: Import failed: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/sqoop/import already exists</p>\n<ul>\n<li>Resolution: When the Target directory (<code>/sqoop/import</code>) already presents in HDFS, then this error comes. So the target-dir name corrected to <code>/sqoop/import/option_type</code> and resolved this issue.</li>\n</ul>\n</li>\n<li><p><strong>Error</strong> 2020-02-20 17:49:09,344 ERROR tool.ImportTool: Import failed: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /tmp/hadoop-yarn/staging/admin/.staging/job_1582220036694_0002. Name node is in safe mode.\nThe reported blocks 0 needs additional 47 blocks to reach the threshold 0.9990 of total blocks 48.\nThe minimum number of live datanodes is not required. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:localhost</p>\n<ul>\n<li>Resolution: Executed the command <code>/usr/local/hadoop/bin/hdfs dfsadmin -safemode leave</code> to release the safemode in namenode.</li>\n</ul>\n</li>\n<li><p><strong>Error</strong> 2020-02-20 19:44:09,389 ERROR tool.ImportTool: Import failed: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /tmp/hadoop-yarn/staging/admin/.staging/job_1582220036694_0003/libjars/commons-io-1.4.jar could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.</p>\n<ul>\n<li>Resolution: Deleted the datanode and namenode directory. Created new namenode directory, formatted it, and created new datanode directory. This step resolved this issue.</li>\n</ul>\n</li>\n<li><p><strong>Error</strong> 2020-02-20 19:54:39,556 ERROR tool.ImportTool: Import failed: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /tmp/hadoop-yarn/staging/admin/.staging/job_1582228326297_0001/libjars/commons-io-1.4.jar could only be written to 0 of the 1 minReplication nodes. There are 1 datanode(s) running and 1 node(s) are excluded in this operation.</p>\n<ul>\n<li>Resolution: Check <a href=\"http://localhost:9870\">http://localhost:9870</a> - Hadoop Admin whether data nodes are running and showing up in datanode tab. If not, restart the dfs and yarn using the same unix account where the process need to run, not as root account.</li>\n</ul>\n</li>\n</ul>\n"
}
