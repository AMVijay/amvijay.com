<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-27752939-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-27752939-2');
</script>
<!-- Meta Tags --> 
<title>Apache Spark Installation And Setup Steps</title>
<meta name="description" content="Apache Spark Installation and Setup in Linux Debian/Ubuntu" />
</head>
<body style="font-family:San-Serif">
<h1 id="apache-spark-installation-in-linux">Apache Spark Installation in Linux</h1>
<p>Last updated : 01/20/2020</p>
<h2 id="install-python-3-x-version">Install Python 3.x version</h2>
<ul>
<li>Check whether Python is installed already in machine using command <code>python3 -V</code> or <code>python3 --version</code>, if not install the python 3.x version using command <code>sudo apt install python3</code>. If need to check what version of python available now in repository, command <code>apt search python3</code> can be used to check the version.</li>
<li>Check whether PIP is installed already in machine using command <code>pip3 --version</code> or <code>python3 -m pip --version</code>. If not, install using command <code>sudo apt install python3-pip</code>.<ul>
<li>PIP can be upgraded to latest version using <code>sudo python3 -m pip install --user --upgrade pip</code></li>
</ul>
</li>
<li>Create Python Virutal Environment using command <code>python3 -m venv /home/admin/virualenv</code>, Here m denotes execute python module.<ul>
<li>Python3.4 and above comes with venv (virtualenv) library in-built. User can create virutalenv using above stated command.</li>
</ul>
</li>
<li>To know the the package version installed in python, use command <code>pip3 show &lt;package_name&gt;</code>, ex: <code>pip3 show virtualenv</code></li>
<li>Activate the python virutal env using command <code>source /home/admin/virtualenv/bin/activate</code></li>
</ul>
<h2 id="install-pyspark">Install PySpark</h2>
<p>After activating the virtualenv as Stated Above, </p>
<ul>
<li>Install PySpark using command <code>pip install pyspark</code>. PySpark would be installed in the virutalenv.</li>
<li>Check the installed package and its version using command <code>pip list</code></li>
<li>Upgrade pip version in virtual env using command <code>pip install --upgrade pip</code></li>
</ul>
<h2 id="install-apache-spark">Install Apache Spark</h2>
<ul>
<li>Use command <code>wget --no-check-certificate https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</code><ul>
<li><code>no-check-certificate</code> is to ignore SSL Certificate verification.</li>
</ul>
</li>
<li>Extract the spark binary from tar.gz file using command <code>tar -xzvf spark-2.4.4-bin-hadoop2.7.tgz</code></li>
<li>Move this spark directory to <code>sudo mv spark-2.4.4-bin-hadoop2.7 /usr/local/spark-2.4.4-bin-hadoop2.7</code></li>
<li>Create softlink using command <code>sudo ln -s /usr/local/spark-2.4.4-bin-hadoop2.7/ /usr/local/spark</code></li>
<li>Update below listed environment path to user home directory <code>.bashrc</code><pre><code class="lang-bash"><span class="hljs-built_in">export</span> SPARK_HOME=/usr/<span class="hljs-built_in">local</span>/spark
<span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$SPARK_HOME</span>/bin:<span class="hljs-variable">$SPARK_HOME</span>/sbin
</code></pre>
</li>
</ul>
<h2 id="launching-apache-spark-master-and-worker-nodes">Launching Apache Spark Master and Worker Nodes</h2>
<ul>
<li>Add JAVA_HOME in <code>sudo nano /usr/local/spark/sbin/spark-config.sh</code><pre><code class="lang-bash"><span class="hljs-built_in">export</span> JAVA_HOME=/usr/<span class="hljs-built_in">local</span>/jdk8
</code></pre>
</li>
<li>Use command to run master node <code>sudo /usr/local/spark/sbin/start-master.sh</code>. Once started successfully, we can access master web ui using command <a href="http://localhost:8080/">http://localhost:8080/</a></li>
<li>Use command to run worker node <code>sudo /usr/local/spark/sbin/start-slave.sh spark://LP0039973.tmm.na.corp.toyota.com:7077 --memory 2G</code>. Once started successfully, check the master webui. Now worker section will show the new worker with IP address and memory.</li>
</ul>
<h1 id="acronyms">Acronyms</h1>
<ul>
<li>apt - Advanced Packing Tool</li>
<li>pip - PIP Installs Package</li>
</ul>
</body>
</html>
