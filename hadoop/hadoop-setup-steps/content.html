<h1 id="hadoop-installation-and-setup">Hadoop Installation and Setup</h1>
<p>Last Updated On 01/16/2020</p>
<h2 id="linux-update-to-the-latest-packages">Linux Update to the latest packages</h2>
<p><code>sudo apt update</code><br><code>sudo apt upgrade</code>  </p>
<h2 id="install-ssh-in-linux">Install SSH in Linux</h2>
<p><code>sudo apt-get install ssh</code> 
<code>sudo apt-get install ca-certificates</code>
<code>sudo service ssh restart</code></p>
<h2 id="update-hadoop-installation">Update Hadoop Installation</h2>
<p><code>sudo apt install wget</code><br><code>wget &lt;hadoop tar http path&gt;</code> - If required perform checksum for reliability, if required add option <code>--no-check-certificate</code><br><code>tar -xzvf &lt;hadoop tar file&gt;</code>   (x - extract, z - use gzip, v - verbose, f - file)<br><code>sudo mv &lt;hadoop extracted folder&gt; /usr/local/hadoop</code>  </p>
<h2 id="install-update-jdk-jre">Install/Update JDK/JRE</h2>
<p><code>sudo apt install default-jre</code> - This normally installs the Java in /usr/lib/jvm/<jdk folder>, and updates the softlink of /usr/bin/java to the installable.
<code>sudo apt install default-jdk</code> -<br><code>sudo update-java-alternatives --list</code> or  <code>sudo update-alternatives --config java</code> - (This is list down all java installable and its path in the os, we can select default JRE in the OS for all users)<br>Or<br>to Install precise Java Version, mention the java version as stated below.<br><code>wget --no-check-certificate https://corretto.aws/downloads/latest/amazon-corretto-8-x64-linux-jdk.tar.gz</code>
<code>tar -xzvf amazon-corretto-8-x64-linux-jdk.tar.gz</code>
<code>sudo mv amazon-corretto-8.242.07.1-linux-x64/ /usr/local/jdk8</code>
Update <code>sudo nano /etc/profile</code> as below, so that JAVA_HOME will set in OS level.</p>
<pre><code class="lang-bash"><span class="hljs-comment"># JAVA PATH Settings</span>
<span class="hljs-built_in">export</span> JAVA_HOME=/usr/<span class="hljs-built_in">local</span>/jdk8
<span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$JAVA_HOME</span>/bin
</code></pre>
<h2 id="update-bashrc-in-user-profile-as-below">Update .bashrc in user profile as below</h2>
<pre><code class="lang-bash"><span class="hljs-comment"># Added for HADOOP</span>
<span class="hljs-built_in">export</span> JAVA_HOME=/usr/<span class="hljs-built_in">local</span>/jdk8
<span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$JAVA_HOME</span>/bin

<span class="hljs-built_in">export</span> HADOOP_HOME=/usr/<span class="hljs-built_in">local</span>/hadoop
<span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$HADOOP_HOME</span>/bin:<span class="hljs-variable">$HADOOP_HOME</span>/sbin
</code></pre>
<p>Execute command <code>source .bashrc</code> - This command will apply all the changes to current running bash session. Source command will make all the changes applied to current running session.</p>
<h2 id="update-hadeep-env-sh-in-hadoop">Update hadeep-env.sh in Hadoop</h2>
<p><code>sudo nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh</code><br><code>export JAVA_HOME=/usr/local/jdk8</code>
<code>export HADOOP_HOME=/usr/local/hadoop/</code>  </p>
<p>Added the below listed one to the /usr/local/hadoop/etc/hadoop/hadoop-env.sh </p>
<pre><code class="lang-bash"><span class="hljs-keyword">export</span> HDFS_NAMENODE_USER=admin
<span class="hljs-keyword">export</span> HDFS_DATANODE_USER=admin
<span class="hljs-keyword">export</span> HDFS_SECONDARYNAMENODE_USER=admin
<span class="hljs-keyword">export</span> YARN_RESOURCEMANAGER_USER=admin
<span class="hljs-keyword">export</span> YARN_NODEMANAGER_USER=admin
</code></pre>
<h2 id="run-mapreduce-algorithm">Run MapReduce Algorithm</h2>
<p><code>mkdir ~/input</code><br><code>cp /usr/local/hadoop/etc/hadoop/*.xml ~/input</code><br><code>/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar grep ~/input ~/grep_example &#39;allowed[.]*&#39;</code><br>    If ~/grep_example already exists, then delete the folder <code>rm -ef ~/grep_example</code>  </p>
<p><code>cat ~/grep_example/*</code> to check the output of mapandreduce  </p>
<h2 id="ssh-configuration">SSH Configuration</h2>
<p><code>sudo apt install openssh-server openssh-client</code><br><code>ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa</code><br><code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code><br><code>chmod 0600 ~/.ssh/authorized_keys</code></p>
<h2 id="modify-configuration">Modify Configuration</h2>
<ul>
<li>Change <code>sudo nano /usr/local/hadoop/etc/hadoop/core-site.xml</code> as  <pre><code class="lang-xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://localhost:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span>
</code></pre>
</li>
<li>Change <code>sudo nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml</code> as  <pre><code class="lang-xml"><span class="hljs-params">&lt;configuration&gt;</span>
  <span class="hljs-params">&lt;property&gt;</span>
      <span class="hljs-params">&lt;name&gt;</span>dfs.replication<span class="hljs-params">&lt;/name&gt;</span>
      <span class="hljs-params">&lt;value&gt;</span><span class="hljs-number">1</span><span class="hljs-params">&lt;/value&gt;</span>
  <span class="hljs-params">&lt;/property&gt;</span>
  <span class="hljs-params">&lt;property&gt;</span>
      <span class="hljs-params">&lt;name&gt;</span>dfs.namenode.name.dir<span class="hljs-params">&lt;/name&gt;</span>
      <span class="hljs-params">&lt;value&gt;</span><span class="hljs-meta-keyword">/usr/</span>local<span class="hljs-meta-keyword">/hadoop/</span>data/namenode<span class="hljs-params">&lt;/value&gt;</span>
  <span class="hljs-params">&lt;/property&gt;</span>
  <span class="hljs-params">&lt;property&gt;</span>
      <span class="hljs-params">&lt;name&gt;</span>dfs.datanode.data.dir<span class="hljs-params">&lt;/name&gt;</span>
      <span class="hljs-params">&lt;value&gt;</span><span class="hljs-meta-keyword">/usr/</span>local<span class="hljs-meta-keyword">/hadoop/</span>data/datanode<span class="hljs-params">&lt;/value&gt;</span>
  <span class="hljs-params">&lt;/property&gt;</span>
<span class="hljs-params">&lt;/configuration&gt;</span>
</code></pre>
</li>
<li>Change <code>sudo nano /usr/local/hadoop/etc/hadoop/yarn-site.xml</code> as <pre><code class="lang-xml"><span class="hljs-params">&lt;configuration&gt;</span>
 <span class="hljs-params">&lt;property&gt;</span>
      <span class="hljs-params">&lt;name&gt;</span>mapreduce.framework.name<span class="hljs-params">&lt;/name&gt;</span>
      <span class="hljs-params">&lt;value&gt;</span>yarn<span class="hljs-params">&lt;/value&gt;</span>
 <span class="hljs-params">&lt;/property&gt;</span>
 <span class="hljs-params">&lt;property&gt;</span>
      <span class="hljs-params">&lt;name&gt;</span>mapreduce.application.classpath<span class="hljs-params">&lt;/name&gt;</span>
      <span class="hljs-params">&lt;value&gt;</span>$HADOOP_MAPRED_HOME<span class="hljs-meta-keyword">/share/</span>hadoop<span class="hljs-meta-keyword">/mapreduce/</span>*:$HADOOP_MAPRED_HOME<span class="hljs-meta-keyword">/share/</span>hadoop<span class="hljs-meta-keyword">/mapreduce/</span>lib<span class="hljs-comment">/*&lt;/value&gt;
 &lt;/property&gt;
&lt;/configuration&gt;</span>
</code></pre>
</li>
<li>Change <code>sudo nano /usr/local/hadoop/etc/hadoop/mapred-site.xml</code> as <pre><code class="lang-xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span>
</code></pre>
</li>
</ul>
<h2 id="start-stop-the-hdfs-file-system">Start/stop the hdfs file system</h2>
<p><code>sudo /usr/local/hadoop/bin/hdfs namenode -format</code> - format the namenode<br><code>sudo /usr/local/hadoop/sbin/start-dfs.sh</code><br><code>sudo /usr/local/hadoop/sbin/stop-dfs.sh</code>  </p>
<p>By default, HDFS NameNode can be accessed using <a href="http://localhost:9870/">http://localhost:9870/</a></p>
<h2 id="start-stop-the-yarn">Start/Stop the YARN</h2>
<p><code>sudo /usr/local/hadoop/sbin/start-yarn.sh</code><br><code>sudo /usr/local/hadoop/sbin/stop-yarn.sh</code></p>
<p>By default, yarn resource manager can be access in <a href="http://localhost:8088/">http://localhost:8088/</a></p>
